{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sqlite3\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from stop_words import get_stop_words\n",
    "import shap\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "romanian_texts = {}\n",
    "moldavian_texts = {}\n",
    "\n",
    "conn = sqlite3.connect('news.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('SELECT * FROM romania')\n",
    "rows = c.fetchall()\n",
    "for row in rows:\n",
    "    if row[4] not in romanian_texts:\n",
    "        romanian_texts[row[4]] = []\n",
    "    romanian_texts[row[4]].append(row[5])\n",
    "    \n",
    "c.execute('SELECT * FROM moldova WHERE newspaper != \"zugo\"')\n",
    "rows = c.fetchall()\n",
    "for row in rows:\n",
    "    text = ''\n",
    "    if len(row[5]) > 10000:\n",
    "        text = row[5][:10000]\n",
    "    else:\n",
    "        text = row[5]\n",
    "    if row[4] not in moldavian_texts:\n",
    "        moldavian_texts[row[4]] = []\n",
    "        \n",
    "    moldavian_texts[row[4]].append(text)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = {\"romana\": [], \"moldova\": []}\n",
    "\n",
    "for key in romanian_texts:\n",
    "    all_texts[\"romana\"].extend(romanian_texts[key])\n",
    "\n",
    "    \n",
    "for key in moldavian_texts:\n",
    "    all_texts[\"moldova\"].extend(moldavian_texts[key])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the study, we have a model with a RoBERT base generating representations for an array of tokens some of them being masked. This first step is followed by a geolocation prediction that feeds the representations into a classification head that predicts the location of the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer, BertModel\n",
    "\n",
    "# https://github.com/valentinhofmann/geoadaptation/blob/main/src/model_geoadaptation.py\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, head, output_dim, config):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, output_dim)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x[:, 0, :] # Use [CLS] token's representation\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "        \n",
    "\n",
    "class GeoadaptedRobert(nn.Module):\n",
    "    def __init__(self, model_name=\"readerbench/RoBERT-large\"):\n",
    "        super(GeoadaptedRobert, self).__init__()\n",
    "        self.robert = AutoModel.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "        # Geolocation Head: 1 value, the probability of the text being from Moldova (0) or Romania (1)\n",
    "        self.geo_head = Classifier(1, self.robert.config)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        # Forward pass through the RoBERT model\n",
    "        outputs = self.robert(input_ids=input_ids, \n",
    "                              attention_mask=attention_mask,\n",
    "                              token_type_ids=token_type_ids)\n",
    "        \n",
    "        # Get the hidden states from the last layer\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "\n",
    "        # MLM Loss\n",
    "        scores = self.cls(sequence_output)\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "        mlm_loss = self.loss_fct(scores.view(-1, scores.size(-1)), labels.view(-1))\n",
    "\n",
    "        # Geolocation Loss\n",
    "        geo_logits = self.geo_head(sequence_output)\n",
    "        self.geo_loss_fct = nn.L1Loss()\n",
    "        geo_loss = self.geo_loss_fct(geo_logits.view(-1), labels.view(-1))\n",
    "        preds = geo_logits.detach().cpu().tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        return mlm_loss, geo_loss, preds\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Training\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"readerbench/RoBERT-large\")\n",
    "\n",
    "colator "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
