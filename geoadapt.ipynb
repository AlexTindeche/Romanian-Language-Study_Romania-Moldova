{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sqlite3\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from stop_words import get_stop_words\n",
    "import shap\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "romanian_texts = {}\n",
    "moldavian_texts = {}\n",
    "\n",
    "conn = sqlite3.connect('news.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('SELECT * FROM romania')\n",
    "rows = c.fetchall()\n",
    "for row in rows:\n",
    "    if row[4] not in romanian_texts:\n",
    "        romanian_texts[row[4]] = []\n",
    "    romanian_texts[row[4]].append(row[5])\n",
    "    \n",
    "c.execute('SELECT * FROM moldova WHERE newspaper != \"zugo\"')\n",
    "rows = c.fetchall()\n",
    "for row in rows:\n",
    "    text = ''\n",
    "    if len(row[5]) > 10000:\n",
    "        text = row[5][:10000]\n",
    "    else:\n",
    "        text = row[5]\n",
    "    if row[4] not in moldavian_texts:\n",
    "        moldavian_texts[row[4]] = []\n",
    "        \n",
    "    moldavian_texts[row[4]].append(text)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = {\"romana\": [], \"moldova\": []}\n",
    "\n",
    "for key in romanian_texts:\n",
    "    all_texts[\"romana\"].extend(romanian_texts[key])\n",
    "\n",
    "    \n",
    "for key in moldavian_texts:\n",
    "    all_texts[\"moldova\"].extend(moldavian_texts[key])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Real Madrid a castigat Supercupa Spaniei, dupa ce in finala a invins-o pe Barcelona, scor 4-1.Madrilenii au avut un start de meci excelent, cu doua goluri marcate in doar trei minute de Vinicius. Brazilianul a reusit hat-trick-ul in minutul 39, dupa ce a transformat un penalty. Partida din Supercupa Spaniei este cel de-al 15-lea \"El Clasico\" pentru Vinicius. Brazilianul a reusit o performanta importanta pentru cariera sa. Starul lui Real Madrid este cel de-al 16-lea jucator din istorie care a marcat de trei ori intr-un \"El Clasico\". Pe aceasta lista se mai afla: Jaime Lazcano, Joan Ramon i Pera, Ventora, Jesus Narro, Cesar, Evaristo de Macedo, Amancio, Ferenc Puskas, Ivan Zamorano, Fernando Sanudo, Gary Lineker, Romario, Luis Suarez, Karim Benzema si Lionel Messi. Este important de precizat ca Messi este singurul fotbalist dintre cei enumerati care a reusit aceasta performanta de doua ori in cariera, potrivit Fanatik. Partida din Supercupa Spaniei este cel de-al 15-lea \"El Clasico\" pentru Vinicius. Brazilianul a reusit o performanta importanta pentru cariera sa. Starul lui Real Madrid este cel de-al 16-lea jucator din istorie care a marcat de trei ori intr-un \"El Clasico\". Pe aceasta lista se mai afla: Jaime Lazcano, Joan Ramon i Pera, Ventora, Jesus Narro, Cesar, Evaristo de Macedo, Amancio, Ferenc Puskas, Ivan Zamorano, Fernando Sanudo, Gary Lineker, Romario, Luis Suarez, Karim Benzema si Lionel Messi. Este important de precizat ca Messi este singurul fotbalist dintre cei enumerati care a reusit aceasta performanta de doua ori in cariera, potrivit Fanatik. Pe aceasta lista se mai afla: Jaime Lazcano, Joan Ramon i Pera, Ventora, Jesus Narro, Cesar, Evaristo de Macedo, Amancio, Ferenc Puskas, Ivan Zamorano, Fernando Sanudo, Gary Lineker, Romario, Luis Suarez, Karim Benzema si Lionel Messi. Este important de precizat ca Messi este singurul fotbalist dintre cei enumerati care a reusit aceasta performanta de doua ori in cariera, potrivit Fanatik. Este important de precizat ca Messi este singurul fotbalist dintre cei enumerati care a reusit aceasta performanta de doua ori in cariera, potrivit Fanatik.\n",
      " Toti specialistii spun ca apa este cea mai buna bautura din lume. Fie ca vorbim de apa minerala sau plata, oamenii are trebuie sa o bea an cantitate mare. Ansa, apele difera si ele an functie de locul de provenienta, lista de ingrediente si pret. Una dintre apele minerale, care este apreciata la nivel international, se vinde si an Rominia si se gaseste pe rafturile magazinelor Kaufland. Ce contine aceasta apa si de ce costa 11 lei doar 0.33 litri?      An magazinele Kaufland, rominii pot gasi una dintre cele mai apreciate ape minerale, la nivel modial. Este vorba despre apa Vichy Catalan, care se vinde cu 11 lei sticla de 0.33 litri. Aceasta apa este un speciala, bogata an minerale, recomandata an special sportivilor.                  Locul de munca pe care al poti avea fara studii superioare, platit cu...                   O recunoasteti? Acum este o femeie extrem de cunoscuta an Rominia!      Vichy Catalan este una dintre cele mai apreciate ape /Foto: Facebook (CITESTE SI: CIT A PLATIT UN SIBIAN PE 3 PAHARE DE APA LA UN RESTAURANT DE FITE DIN ORASUL DE PE CIBIN. CIND A VAZUT NOTA DE PLATA NU I-A VENIT SA CREADA) Ce contine, de fapt, apa minerala Vichy Catalan Vichy Catalan este una dintre cele mai cunoscute ape minerale spumante din Spania. Aceasta a fost descoperita de doctorul Modest Furest si pusa spre vinzare pe ‪piata an anul 1883‬, fiind una dintre cele mai cunoscute ape europene. Aceasta a obtinut an anul 2021 locul 1 la categoria cea mai buna apa minerala naturala din Europa, an cadrul premiilor ,,Food&Drink”. Apa Vichy Catalan este bogat an minerale si ajuta la digestie prin continutul ridicat de bicarbonati si sodiu. S-a demonstrat ca aceasta reduce colesterolul si aduce beneficii inimii. Mai mult, contine 27 din cele 34 de minerale de care corpul uman are nevoie pentru o sanatate buna. De asemenea, despre aceasta apa minerala se spune ca previne cariile dentare, ajuta la antarirea structurii osoase, dar si ca are efecte antidepresive datorita continutului de litiu. Cu o cantitate surprinzatoare de solide total dizolvate (3052 mg pe Litru), Vichy Catalan este o apa bogata an toate mineralele, cu exceptia nitratilor care sunt aproape de zero. Este bogata an calciu, magneziu, sodiu, potasiu, bicarbonat, fluorura si siciliu. Aceasta apa consumata regulat reduce colesterolul si este perfecta pentru rehidratarea dupa practicarea oricarui sport Asa cum spuneam, apa care se vinde cu 11 lei sticla de 0.33 litri are numeroase minerale. Compozitie/mg/1000 ml: TDS 2900mg, SILICA 15mg, BICARBONAT 2081mg, SULFAT 46mg, CLORURA 680mg, POTASIU 44mg, SODIU 1100mg, MAGNEZIU 6mg, CALCIU 14mg, NITRAT 1mg, DURITATE 61mg, PH 8.      Vichy Catalan este o apa bogata an minerale /Foto: Facebook (VEZI SI: CE CONTIN URECHILE DE PORC FELIATE DIN KAUFLAND. PENTRU MAJORITATEA ROMINILOR, SUNT O DELICATESA)              \n"
     ]
    }
   ],
   "source": [
    "# De aici: https://en.wiktionary.org/wiki/Category:Romanian_prefixes\n",
    "romanian_prefixes = [\n",
    "    # A\n",
    "    \"agro\", \"alt\", \"ante\", \"anti\", \"aorto\", \"arhi\", \"astro\",\n",
    "\n",
    "    # B\n",
    "    \"balano\",\n",
    "\n",
    "    # C\n",
    "    \"cardio\", \"carpo\", \"cosmo\",\n",
    "\n",
    "    # D\n",
    "    \"demono\", \"des\", \"dez\",\n",
    "\n",
    "    # F\n",
    "    \"franco\",\n",
    "\n",
    "    # G\n",
    "    \"gastro\", \"germano\", \"greco\",\n",
    "\n",
    "    # H\n",
    "    \"hecto\", \"hiper\",\n",
    "\n",
    "    # I\n",
    "    \"în\",\n",
    "\n",
    "    # K\n",
    "    \"kilo\",\n",
    "\n",
    "    # L\n",
    "    \"lexico\",\n",
    "\n",
    "    # M\n",
    "    \"mili\", \"muzico\",\n",
    "\n",
    "    # N\n",
    "    \"nano\", \"ne\",\n",
    "\n",
    "    # O\n",
    "    \"ori\", \"ornito\",\n",
    "\n",
    "    # P\n",
    "    \"pneumo\", \"pre\", \"prea\", \"proto\", \"pseudo\", \"psiho\",\n",
    "\n",
    "    # R\n",
    "    \"răs\", \"re\", \"rino\", \"ruso\",\n",
    "\n",
    "    # S\n",
    "    \"stră\", \"sub\",\n",
    "\n",
    "    # T\n",
    "    \"tehno\", \"teo\", \"termo\",\n",
    "\n",
    "    # V\n",
    "    \"vice\"\n",
    "]\n",
    "\n",
    "\n",
    "def replace_i_prefix(word, prefixes):\n",
    "  for prefix in prefixes:\n",
    "    try:\n",
    "      if word.lower().startswith(prefix) and len(word) > len(prefix) and word[len(prefix):][0] in [\"î\", \"Î\"]:\n",
    "        first_letter = word[len(prefix):][0]\n",
    "        first_letter = \"i\" if first_letter == \"î\" else (\"I\" if first_letter == \"Î\" else first_letter)\n",
    "        word = prefix + first_letter + word[len(prefix) + 1:]\n",
    "\n",
    "    except:\n",
    "      print(word)\n",
    "    \n",
    "  word = word.replace(\"î\", \"a\").replace(\"Î\", \"A\")\n",
    "\n",
    "  return word\n",
    "\n",
    "def no_diacritics(text, prefixes):\n",
    "\n",
    "  text = replace_i_prefix(text, prefixes)\n",
    "\n",
    "\n",
    "  text = text.replace(\"â\", \"i\")\n",
    "  text = text.replace(\"Â\", \"I\")\n",
    "  text = text.replace(\"ș\", \"s\")\n",
    "  text = text.replace(\"ş\", \"s\")\n",
    "  text = text.replace(\"Ș\", \"S\")\n",
    "  text = text.replace(\"Ş\", \"S\")\n",
    "  text = text.replace(\"ț\", \"t\")\n",
    "  text = text.replace(\"ţ\", \"t\")\n",
    "  text = text.replace(\"Ț\", \"T\")\n",
    "  text = text.replace(\"Ţ\", \"T\")\n",
    "\n",
    "  # If î is the first letter of the word, replace it with i\n",
    "  if text.startswith(\"î\"):\n",
    "    text = text.replace(\"î\", \"i\")\n",
    "  if text.startswith(\"Î\"):\n",
    "    text = text.replace(\"Î\", \"I\")\n",
    "  # If the last letter of the word is î, replace it with i\n",
    "  if text.endswith(\"î\"):\n",
    "    text = text.replace(\"î\", \"i\")\n",
    "  if text.endswith(\"Î\"):\n",
    "    text = text.replace(\"Î\", \"I\")\n",
    "  # Else replace î with a\n",
    "  if \"î\" in text:\n",
    "    text = text.replace(\"î\", \"a\")     \n",
    "  # text = text.replace(\"î\", \"i\")\n",
    "  # text = text.replace(\"Î\", \"I\")\n",
    "  text = text.replace(\"ă\", \"a\")\n",
    "  text = text.replace(\"Ă\", \"A\")\n",
    "\n",
    "  return text\n",
    "\n",
    "\n",
    "for key in moldavian_texts:\n",
    "    for i in range(len(moldavian_texts[key])):\n",
    "        moldavian_texts[key][i] = no_diacritics(moldavian_texts[key][i], romanian_prefixes)\n",
    "\n",
    "for key in romanian_texts:\n",
    "    for i in range(len(romanian_texts[key])):\n",
    "        romanian_texts[key][i] = no_diacritics(romanian_texts[key][i], romanian_prefixes)\n",
    "\n",
    "print(moldavian_texts[\"Sport\"][0])\n",
    "print(romanian_texts['Stiri'][12])\n",
    "\n",
    "# print(no_diacritics(\"cîțiva\", romanian_prefixes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the study, we have a model with a RoBERT base generating representations for an array of tokens some of them being masked. This first step is followed by a geolocation prediction that feeds the representations into a classification head that predicts the location of the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer, BertModel\n",
    "\n",
    "# https://github.com/valentinhofmann/geoadaptation/blob/main/src/model_geoadaptation.py\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, output_dim, config):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, output_dim)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[:, 0, :]  # Use [CLS] token's representation\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)  # Output logits for each class (Romania, Moldova)\n",
    "        return x\n",
    "\n",
    "        \n",
    "\n",
    "class GeoadaptedRobert(nn.Module):\n",
    "    def __init__(self, head, model_name=\"readerbench/RoBERT-large\"):\n",
    "        super(GeoadaptedRobert, self).__init__()\n",
    "        self.robert = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Geolocation Head: 2 values, one for each class (Romania, Moldova)\n",
    "        self.geo_head = Classifier(output_dim=2, config=self.robert.config)\n",
    "\n",
    "        # MLM head (assuming you already have a classifier head for MLM)\n",
    "        self.cls = nn.Linear(self.robert.config.hidden_size, self.robert.config.vocab_size)\n",
    "\n",
    "        self.head = head\n",
    "\n",
    "    def forward(self, input_ids, head, attention_mask=None, token_type_ids=None, labels=None, points=None, val=None):\n",
    "        # Forward pass through the RoBERT model\n",
    "        outputs = self.robert(input_ids=input_ids, \n",
    "                              attention_mask=attention_mask,\n",
    "                              token_type_ids=token_type_ids)\n",
    "        \n",
    "        # Get the hidden states from the last layer\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "\n",
    "        # MLM Loss\n",
    "        scores = self.cls(sequence_output)\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "        mlm_loss = self.loss_fct(scores.view(-1, scores.size(-1)), labels.view(-1))\n",
    "\n",
    "        # Geolocation Classification Loss\n",
    "        geo_logits = self.geo_head(sequence_output)\n",
    "        if head == 'masked':\n",
    "            geo_logits = geo_logits[labels != -100]\n",
    "        self.geo_loss_fct = nn.CrossEntropyLoss()\n",
    "        geo_loss = self.geo_loss_fct(geo_logits, points)  # 'points' should be class labels (0 for Moldova, 1 for Romania)\n",
    "        preds = torch.argmax(geo_logits, dim=-1)\n",
    "\n",
    "        return mlm_loss, geo_loss, preds\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Training\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collator\n",
    "\n",
    "# https://github.com/valentinhofmann/geoadaptation/blob/main/src/helpers_geoadaptation.py#L25\n",
    "class MLMCollator:\n",
    "    def __init__(self, tokenizer, head, probab = 0.15):\n",
    "        self.tok = tokenizer\n",
    "        self.mlm_probability = probab\n",
    "        self.head = head\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        texts = [self.tok.encode(text, padding = True, truncation = True) for text, _ in batch]\n",
    "        points = torch.tensor([point for _, point in batch], dtype=torch.float32)\n",
    "        batch_size = len(texts)\n",
    "        max_len = max(len(text) for text in texts)\n",
    "        input_ids = torch.zeros(batch_size, max_len).long()\n",
    "        attention_mask = torch.zeros(batch_size, max_len).long()\n",
    "        token_type_ids = torch.zeros(batch_size, max_len).long()\n",
    "        for i, text in enumerate(texts):\n",
    "            input_ids[i, :len(text)] = torch.tensor(text)\n",
    "            attention_mask[i, :len(text)] = 1\n",
    "        batch_tensors = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'token_type_ids': token_type_ids\n",
    "        }\n",
    "\n",
    "        # https://github.com/huggingface/transformers/blob/master/src/transformers/data/data_collator.py\n",
    "        mlm_labels = batch_tensors['input_ids'].clone()\n",
    "        probability_matrix = torch.full(mlm_labels.shape, self.mlm_probability)\n",
    "        special_tokens_mask = [\n",
    "            self.tok.get_special_tokens_mask(val, already_has_special_tokens=True) for val in mlm_labels.tolist()\n",
    "        ]\n",
    "        special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        mlm_labels[~masked_indices] = -100\n",
    "        indices_replaced = torch.bernoulli(torch.full(mlm_labels.shape, 0.8)).bool() & masked_indices\n",
    "        batch_tensors['input_ids'][indices_replaced] = self.tok.convert_tokens_to_ids(self.tok.mask_token)\n",
    "        indices_random = torch.bernoulli(torch.full(mlm_labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tok), mlm_labels.shape, dtype=torch.long)\n",
    "        batch_tensors['input_ids'][indices_random] = random_words[indices_random]\n",
    "\n",
    "        # Repeat points for masked tokens\n",
    "        if self.head == 'masked':\n",
    "            n_masks = masked_indices.sum(axis=-1)\n",
    "            points = torch.repeat_interleave(points, n_masks, dim=0)\n",
    "        # print(type(batch_tensors), type(mlm_labels), type(points))\n",
    "        return batch_tensors, mlm_labels, points\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Procuratu [1. 0.]\n",
      " Meciul de [0. 1.]\n",
      "814 81 204\n",
      "814 81 204\n",
      "2520 252 631\n",
      "2520 252 631\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "class data:\n",
    "    def __init__(self, texts, points):\n",
    "        self.texts = texts\n",
    "        self.points = points\n",
    "\n",
    "moldavian_texts = [text for text in all_texts[\"moldova\"]]\n",
    "moldavian_points = np.zeros((len(moldavian_texts), 2))\n",
    "moldavian_points[:, 0] = 1\n",
    "\n",
    "romanian_texts = [text for text in all_texts[\"romana\"]]\n",
    "romanian_points = np.zeros((len(romanian_texts), 2))\n",
    "romanian_points[:, 1] = 1\n",
    "\n",
    "class DatasetForMaskedLM:\n",
    "    def __init__(self, data, scaler = None):\n",
    "        self.texts = data.texts\n",
    "        if scaler is None:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.points = self.scaler.fit_transform(data.points)\n",
    "        else:\n",
    "            self.scaler = scaler\n",
    "            self.points = self.scaler.transform(data.points)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.points[idx]\n",
    "\n",
    "# Slect random texts for training and testing\n",
    "moldavian_texts_train, moldavian_texts_test, moldavian_points_train, moldavian_points_test = train_test_split(moldavian_texts, moldavian_points, test_size=0.2, random_state=42)\n",
    "romanian_texts_train, romanian_texts_test, romanian_points_train, romanian_points_test = train_test_split(romanian_texts, romanian_points, test_size=0.2, random_state=42)\n",
    "\n",
    "moldavian_texts_val = moldavian_texts_train[:len(moldavian_texts_train) // 10]\n",
    "moldavian_points_val = moldavian_points_train[:len(moldavian_texts_train) // 10]\n",
    "\n",
    "romanian_texts_val = romanian_texts_train[:len(romanian_texts_train) // 10]\n",
    "romanian_points_val = romanian_points_train[:len(romanian_texts_train) // 10]\n",
    "\n",
    "# Select a random text and its corresponding label\n",
    "moldavian_text, moldavian_point = moldavian_texts_train[np.random.randint(len(moldavian_texts_train))], moldavian_points_train[np.random.randint(len(moldavian_points_train))]\n",
    "romanian_text, romanian_point = romanian_texts_train[np.random.randint(len(romanian_texts_train))], romanian_points_train[np.random.randint(len(romanian_points_train))]\n",
    "print(moldavian_text[:10], moldavian_point)\n",
    "print(romanian_text[:10], romanian_point)\n",
    "\n",
    "# Create the data objects\n",
    "romanian_data_train = data(romanian_texts_train, romanian_points_train)\n",
    "moldavian_data_train = data(moldavian_texts_train, moldavian_points_train)\n",
    "\n",
    "romanian_data_val = data(romanian_texts_val, romanian_points_val)\n",
    "moldavian_data_val = data(moldavian_texts_val, moldavian_points_val)\n",
    "\n",
    "romanian_data_test = data(romanian_texts_test, romanian_points_test)\n",
    "moldavian_data_test = data(moldavian_texts_test, moldavian_points_test)\n",
    "\n",
    "# Create the datasets\n",
    "romanian_dataset_train = DatasetForMaskedLM(romanian_data_train)\n",
    "moldavian_dataset_train = DatasetForMaskedLM(moldavian_data_train)\n",
    "\n",
    "romanian_dataset_val = DatasetForMaskedLM(romanian_data_val, romanian_dataset_train.scaler)\n",
    "moldavian_dataset_val = DatasetForMaskedLM(moldavian_data_val, romanian_dataset_train.scaler)\n",
    "\n",
    "romanian_dataset_test = DatasetForMaskedLM(romanian_data_test, romanian_dataset_train.scaler)\n",
    "moldavian_dataset_test = DatasetForMaskedLM(moldavian_data_test, romanian_dataset_train.scaler)\n",
    "\n",
    "print(len(romanian_texts_train), len(romanian_texts_val), len(romanian_texts_test))\n",
    "print(len(romanian_points_train), len(romanian_points_val), len(romanian_points_test))\n",
    "\n",
    "print(len(moldavian_texts_train), len(moldavian_texts_val), len(moldavian_texts_test))  \n",
    "print(len(moldavian_points_train), len(moldavian_points_val), len(moldavian_points_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Import DatasetForMaskedLM\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"readerbench/RoBERT-large\")\n",
    "\n",
    "collator = MLMCollator(tokenizer, head='masked', probab=0.15)\n",
    "\n",
    "train_loader = DataLoader(romanian_dataset_train, batch_size=32, shuffle=True, collate_fn=collator)\n",
    "val_loader = DataLoader(romanian_dataset_val, batch_size=32, shuffle=False, collate_fn=collator)\n",
    "test_loader = DataLoader(romanian_dataset_test, batch_size=32, shuffle=False, collate_fn=collator)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GeoadaptedRobert(head='masked', model_name=\"readerbench/RoBERT-large\").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "warmup_steps = 3 * len(train_loader)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda step: min(step / warmup_steps, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([2275, 2])\n"
     ]
    }
   ],
   "source": [
    "# Get the first batch of the train loader\n",
    "batch = next(iter(train_loader))\n",
    "input_ids, mlm_labels, points = batch\n",
    "\n",
    "\n",
    "# Print the shape of the input_ids tensor\n",
    "print(input_ids['input_ids'].shape)\n",
    "print(mlm_labels.shape)\n",
    "print(points.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model...\n",
      "Input IDs shape: torch.Size([32, 512])\n",
      "Attention mask shape: torch.Size([32, 512])\n",
      "Token type IDs shape: torch.Size([32, 512])\n",
      "MLM labels shape: torch.Size([32, 512])\n",
      "Train model...\n",
      "Input IDs shape: torch.Size([32, 512])\n",
      "Attention mask shape: torch.Size([32, 512])\n",
      "Token type IDs shape: torch.Size([32, 512])\n",
      "MLM labels shape: torch.Size([32, 512])\n",
      "Train model...\n",
      "Input IDs shape: torch.Size([32, 512])\n",
      "Attention mask shape: torch.Size([32, 512])\n",
      "Token type IDs shape: torch.Size([32, 512])\n",
      "MLM labels shape: torch.Size([32, 512])\n",
      "Train model...\n",
      "Input IDs shape: torch.Size([32, 512])\n",
      "Attention mask shape: torch.Size([32, 512])\n",
      "Token type IDs shape: torch.Size([32, 512])\n",
      "MLM labels shape: torch.Size([32, 512])\n",
      "Train model...\n",
      "Input IDs shape: torch.Size([32, 512])\n",
      "Attention mask shape: torch.Size([32, 512])\n",
      "Token type IDs shape: torch.Size([32, 512])\n",
      "MLM labels shape: torch.Size([32, 512])\n",
      "Train model...\n",
      "Input IDs shape: torch.Size([32, 512])\n",
      "Attention mask shape: torch.Size([32, 512])\n",
      "Token type IDs shape: torch.Size([32, 512])\n",
      "MLM labels shape: torch.Size([32, 512])\n",
      "Train model...\n",
      "Input IDs shape: torch.Size([32, 512])\n",
      "Attention mask shape: torch.Size([32, 512])\n",
      "Token type IDs shape: torch.Size([32, 512])\n",
      "MLM labels shape: torch.Size([32, 512])\n",
      "Train model...\n",
      "Input IDs shape: torch.Size([32, 512])\n",
      "Attention mask shape: torch.Size([32, 512])\n",
      "Token type IDs shape: torch.Size([32, 512])\n",
      "MLM labels shape: torch.Size([32, 512])\n",
      "Train model...\n",
      "Input IDs shape: torch.Size([32, 512])\n",
      "Attention mask shape: torch.Size([32, 512])\n",
      "Token type IDs shape: torch.Size([32, 512])\n",
      "MLM labels shape: torch.Size([32, 512])\n",
      "Train model...\n",
      "Input IDs shape: torch.Size([32, 512])\n",
      "Attention mask shape: torch.Size([32, 512])\n",
      "Token type IDs shape: torch.Size([32, 512])\n",
      "MLM labels shape: torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "mtl = False\n",
    "\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "for epoch in range(1, 10 + 1):\n",
    "    print('Train model...')\n",
    "    model.train()\n",
    "    for i, (batch_tensors, mlm_labels, points) in enumerate(train_loader):\n",
    "        input_ids = batch_tensors['input_ids'].to(device)\n",
    "        attention_mask = batch_tensors['attention_mask'].to(device)\n",
    "        token_type_ids = batch_tensors['token_type_ids'].to(device)\n",
    "        mlm_labels = mlm_labels.to(device)\n",
    "        # print(input_ids.shape)\n",
    "        # print(attention_mask.shape)\n",
    "        # print(token_type_ids.shape)\n",
    "        # print(mlm_labels.shape)\n",
    "        vocab_size = tokenizer.vocab_size\n",
    "        if torch.max(input_ids) >= vocab_size:\n",
    "            print(\"Error: input_ids contains out-of-bounds token index.\")\n",
    "        if torch.max(mlm_labels) >= vocab_size:\n",
    "            print(\"Error: mlm_labels contains out-of-bounds token index.\")\n",
    "        print(\"Input IDs shape:\", input_ids.shape)\n",
    "        print(\"Attention mask shape:\", attention_mask.shape)\n",
    "        print(\"Token type IDs shape:\", token_type_ids.shape)\n",
    "        print(\"MLM labels shape:\", mlm_labels.shape)\n",
    "\n",
    "\n",
    "        break\n",
    "\n",
    "        if i == 0:\n",
    "            print(input_ids[0, :])\n",
    "        points = points.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        mlm_loss, geo_loss, preds = model(\n",
    "            input_ids,\n",
    "            attention_mask,\n",
    "            token_type_ids,\n",
    "            mlm_labels,\n",
    "            points,\n",
    "            False\n",
    "        )\n",
    "        if mtl:\n",
    "            loss = mlm_loss + geo_loss\n",
    "            loss.backward()\n",
    "        else:\n",
    "            mlm_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
